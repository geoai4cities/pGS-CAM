{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "692b3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac1bb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(100, 200, requires_grad=True)\n",
    "b = torch.pow(a, 2) + 3 * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cff8690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_derivative(f, wrt, n):\n",
    "    for i in range(n):\n",
    "        grads = torch.autograd.grad(f, wrt, create_graph=True)\n",
    "        grads.sum()\n",
    "        f = grads\n",
    "#             f = grads.sum()\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8299846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.6698, 3.8035, 4.2887,  ..., 4.1656, 3.3207, 4.0109],\n",
       "         [3.3827, 4.8748, 4.5205,  ..., 3.7685, 3.6921, 3.3590],\n",
       "         [3.0833, 4.5037, 3.3632,  ..., 4.0045, 3.4554, 4.7202],\n",
       "         ...,\n",
       "         [4.8781, 4.7726, 3.8747,  ..., 3.3481, 3.1356, 3.3988],\n",
       "         [4.6725, 3.1451, 4.4856,  ..., 4.9464, 4.8423, 3.0560],\n",
       "         [3.7187, 3.1807, 4.6813,  ..., 3.0730, 4.6216, 3.9625]],\n",
       "        grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_derivative(b.sum(), a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ae48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98045254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29f613a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1499549/3426192354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/DATA/abhishek/venv/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/abhishek/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "b.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "621381b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.9821, 8.7196, 7.1242,  ..., 8.6208, 8.9928, 8.9673],\n",
       "        [9.6366, 7.2352, 9.4872,  ..., 8.1029, 9.2501, 9.9416],\n",
       "        [8.5739, 8.1250, 7.1124,  ..., 9.8386, 6.8623, 9.0057],\n",
       "        ...,\n",
       "        [9.0648, 8.6513, 9.6277,  ..., 8.7181, 9.0253, 6.4664],\n",
       "        [6.3691, 7.1261, 8.6387,  ..., 7.3853, 6.4720, 6.2660],\n",
       "        [8.2533, 8.4686, 9.4588,  ..., 6.4376, 8.6677, 8.5939]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8086ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
